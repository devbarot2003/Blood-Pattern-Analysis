{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e498c5b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.measure import regionprops, label as sk_label\n",
    "from skimage.morphology import remove_small_objects, binary_opening, binary_closing, disk\n",
    "from skimage.segmentation import slic\n",
    "from skimage.util import img_as_float\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from ipywidgets import Dropdown, Text, Button, VBox, Output\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "DATA_ROOT = r\"C:\\BPA\\data\"\n",
    "\n",
    "\n",
    "CLASS_NAMES = [\"gunshot\", \"impact\"]\n",
    "LABEL_TO_INT = {c: i for i, c in enumerate(CLASS_NAMES)}\n",
    "INT_TO_LABEL = {i: c for c, i in LABEL_TO_INT.items()}\n",
    "\n",
    "print(\"DATA_ROOT =\", DATA_ROOT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab9f53",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 2 — Building image index from DATA_ROOT \n",
    "\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Extensions we consider as images\n",
    "IMG_EXTS = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tif\", \"*.tiff\")\n",
    "\n",
    "\n",
    "def infer_label_from_filename(path: str) -> Tuple[str, int]:\n",
    "    \"\"\"\n",
    "\n",
    "    Assumption given:\n",
    "      - Files starting with 'GS-' (case-insensitive) => gunshot\n",
    "      - Everything else => impact\n",
    "\n",
    "    \"\"\"\n",
    "    name = os.path.basename(path)\n",
    "    upper = name.upper()\n",
    "\n",
    "    if upper.startswith(\"GS-\"):\n",
    "        label_str = \"gunshot\"\n",
    "    else:\n",
    "        label_str = \"impact\"\n",
    "\n",
    "    label_int = LABEL_TO_INT[label_str]\n",
    "    return label_str, label_int\n",
    "\n",
    "\n",
    "def build_image_index(data_root: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scans DATA_ROOT recursively and creates a DataFrame with:\n",
    "      pattern_id, filepath, label, label_int\n",
    "\n",
    "    pattern_id is taken from the filename without extension\n",
    "      (GS-1.jpg -> 'GS-1')\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    root = Path(data_root)\n",
    "\n",
    "    if not root.is_dir():\n",
    "        raise RuntimeError(f\"DATA_ROOT does not exist: {data_root}\")\n",
    "\n",
    "    for ext in IMG_EXTS:\n",
    "        pattern = os.path.join(str(root), \"**\", ext)\n",
    "        files = glob.glob(pattern, recursive=True)\n",
    "        for f in files:\n",
    "            pid = os.path.splitext(os.path.basename(f))[0]\n",
    "            label_str, label_int = infer_label_from_filename(f)\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"pattern_id\": pid,\n",
    "                    \"filepath\": f,\n",
    "                    \"label\": label_str,\n",
    "                    \"label_int\": label_int,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"No images found\")\n",
    "\n",
    "    # Optional: sort for nice, stable ordering\n",
    "    df = df.sort_values(by=[\"label\", \"pattern_id\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "images_df = build_image_index(DATA_ROOT)\n",
    "\n",
    "print(\"Image index built from DATA_ROOT:\")\n",
    "print(f\"  Total images found: {len(images_df)}\\n\")\n",
    "\n",
    "print(\"  Images per class (inferred):\")\n",
    "print(images_df[\"label\"].value_counts())\n",
    "\n",
    "display(images_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb5d3e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 3 — Image loading & visualization \n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "import numpy as np\n",
    "\n",
    "def load_image_bgr(path: str) -> np.ndarray:\n",
    " \n",
    "    pil_img = Image.open(path).convert(\"RGB\")\n",
    "    img_rgb = np.array(pil_img)    # full resolution\n",
    "    img_bgr = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)\n",
    "    return img_bgr\n",
    "\n",
    "\n",
    "#Show a safely resized preview for visualization ONLY.\n",
    "def show_safe_preview(path: str, max_dim: int = 1800):\n",
    "\n",
    "    pil_img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    w, h = pil_img.size\n",
    "    scale = max_dim / max(w, h)\n",
    "    if scale < 1.0:\n",
    "        new_w = int(w * scale)\n",
    "        new_h = int(h * scale)\n",
    "        pil_small = pil_img.resize((new_w, new_h))\n",
    "    else:\n",
    "        pil_small = pil_img\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(pil_small)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(Path(path).name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "test_row = images_df.iloc[0]\n",
    "show_safe_preview(test_row[\"filepath\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5331827e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4 — Simple grayscale-based mask (for contingency seeds)\n",
    "\n",
    "def segment_gray_baseline(img_bgr: np.ndarray,\n",
    "                          min_area: int = 30,\n",
    "                          scale_factor: float = 1.0) -> np.ndarray:\n",
    "\n",
    "    h, w = img_bgr.shape[:2]\n",
    "\n",
    "    # Optionally downscale for speed \n",
    "    if scale_factor != 1.0:\n",
    "        new_w = int(w * scale_factor)\n",
    "        new_h = int(h * scale_factor)\n",
    "        img_proc = cv2.resize(\n",
    "            img_bgr, (new_w, new_h),\n",
    "            interpolation=cv2.INTER_AREA if scale_factor < 1.0 else cv2.INTER_LINEAR\n",
    "        )\n",
    "    else:\n",
    "        img_proc = img_bgr\n",
    "\n",
    "    gray = cv2.cvtColor(img_proc, cv2.COLOR_BGR2GRAY)\n",
    "    inv = 255 - gray\n",
    "\n",
    "    # Otsu threshold\n",
    "    _, thr = cv2.threshold(inv, 0, 255,\n",
    "                           cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    mask_bool = thr.astype(bool)\n",
    "    mask_bool = remove_small_objects(mask_bool, min_size=min_area)\n",
    "    mask_small = (mask_bool.astype(np.uint8) * 255)\n",
    "\n",
    "    # Upscale mask back to original resolution\n",
    "    if scale_factor != 1.0:\n",
    "        mask = cv2.resize(mask_small, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "        mask = (mask > 0).astype(np.uint8) * 255\n",
    "    else:\n",
    "        mask = mask_small\n",
    "\n",
    "    return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c831f7a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5 - Multi pass HSV based blood segmentation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def segment_blood_hsv(\n",
    "    img_bgr: np.ndarray,\n",
    "    base_min_size: int = 5,\n",
    "    debug_show: bool = False,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    " \n",
    "\n",
    "    # Preprocess\n",
    "    img_blur = cv2.GaussianBlur(img_bgr, (3, 3), 0)\n",
    "    hsv = cv2.cvtColor(img_blur, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    # CLAHE on V channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    v_eq = clahe.apply(v)\n",
    "    hsv_eq = cv2.merge([h, s, v_eq])\n",
    "\n",
    "    #Red hue mask \n",
    "    lower_red1 = np.array([0, 30, 10], dtype=np.uint8)\n",
    "    upper_red1 = np.array([12, 255, 255], dtype=np.uint8)\n",
    "    lower_red2 = np.array([160, 30, 10], dtype=np.uint8)\n",
    "    upper_red2 = np.array([180, 255, 255], dtype=np.uint8)\n",
    "\n",
    "    mask1 = cv2.inRange(hsv_eq, lower_red1, upper_red1)\n",
    "    mask2 = cv2.inRange(hsv_eq, lower_red2, upper_red2)\n",
    "    mask_red = cv2.bitwise_or(mask1, mask2)\n",
    "\n",
    "    if mask_red.sum() == 0:\n",
    "        empty = np.zeros(mask_red.shape, dtype=np.uint8)\n",
    "        labels_empty = sk_label(empty.astype(bool), connectivity=2)\n",
    "        if debug_show:\n",
    "            show_image_and_mask(img_bgr, empty, title=\"No red detected\")\n",
    "        return empty, labels_empty\n",
    "\n",
    "    # stats over red pixels\n",
    "    s_red = s[mask_red > 0]\n",
    "    v_red = v_eq[mask_red > 0]\n",
    "    if len(s_red) < 50:\n",
    "        s_red = s.flatten()\n",
    "        v_red = v_eq.flatten()\n",
    "\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    area = H * W\n",
    "\n",
    "    def run_pass(s_thresh, v_thresh, min_size):\n",
    "        sat_ok = (s >= s_thresh).astype(np.uint8) * 255\n",
    "        val_ok = (v_eq >= v_thresh).astype(np.uint8) * 255\n",
    "        m = cv2.bitwise_and(mask_red, sat_ok)\n",
    "        m = cv2.bitwise_and(m, val_ok)\n",
    "\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "        m = cv2.morphologyEx(m, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "        m = cv2.morphologyEx(m, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "\n",
    "        mb = m.astype(bool)\n",
    "        mb = remove_small_objects(mb, min_size=min_size)\n",
    "        return mb\n",
    "\n",
    "    # Percentile thresholds\n",
    "    p_s50 = np.percentile(s_red, 50)\n",
    "    p_s30 = np.percentile(s_red, 30)\n",
    "    p_s15 = np.percentile(s_red, 15)\n",
    "\n",
    "    p_v50 = np.percentile(v_red, 50)\n",
    "    p_v30 = np.percentile(v_red, 30)\n",
    "    p_v15 = np.percentile(v_red, 15)\n",
    "\n",
    "    strict_s = max(30, p_s50)\n",
    "    strict_v = max(25, p_v50)\n",
    "    min_size_strict = max(base_min_size * 4, int(0.000001 * area))\n",
    "\n",
    "    mid_s = max(20, p_s30)\n",
    "    mid_v = max(15, p_v30)\n",
    "    min_size_mid = max(base_min_size * 2, int(0.0000007 * area))\n",
    "\n",
    "    relax_s = max(10, p_s15)\n",
    "    relax_v = max(8,  p_v15)\n",
    "    min_size_relax = max(base_min_size, int(0.0000003 * area))\n",
    "\n",
    "    # Masks\n",
    "    mb_strict = run_pass(strict_s, strict_v, min_size_strict)\n",
    "    mb_mid    = run_pass(mid_s,    mid_v,    min_size_mid)\n",
    "    mb_relax  = run_pass(relax_s,  relax_v,  min_size_relax)\n",
    "\n",
    "    # Combine (union)\n",
    "    mb_combined = mb_strict | mb_mid | mb_relax\n",
    "\n",
    "    mb_final = remove_small_objects(mb_combined, min_size=base_min_size)\n",
    "    mask_out = mb_final.astype(np.uint8) * 255\n",
    "    labels   = sk_label(mb_final, connectivity=2)\n",
    "\n",
    "    if debug_show:\n",
    "        preview = cv2.resize(img_bgr, (1000, 1000 * img_bgr.shape[0] // img_bgr.shape[1]))\n",
    "\n",
    "        strict_mask = mb_strict.astype(np.uint8) * 255\n",
    "        mid_mask    = mb_mid.astype(np.uint8) * 255\n",
    "        relax_mask  = mb_relax.astype(np.uint8) * 255\n",
    "\n",
    "        plt.figure(figsize=(16, 10))\n",
    "\n",
    "        plt.subplot(2, 3, 1)\n",
    "        plt.imshow(cv2.cvtColor(preview, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"Preview\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(2, 3, 2)\n",
    "        plt.imshow(mask_out, cmap=\"gray\")\n",
    "        plt.title(f\"Final mask (stains: {labels.max()})\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(2, 3, 4)\n",
    "        plt.imshow(strict_mask, cmap=\"gray\")\n",
    "        plt.title(\"Strict\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(2, 3, 5)\n",
    "        plt.imshow(mid_mask, cmap=\"gray\")\n",
    "        plt.title(\"Mid\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(2, 3, 6)\n",
    "        plt.imshow(relax_mask, cmap=\"gray\")\n",
    "        plt.title(\"Relaxed\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return mask_out, labels\n",
    "\n",
    "\n",
    "\n",
    "test_row = images_df.iloc[0]\n",
    "test_img = load_image_bgr(test_row[\"filepath\"])  \n",
    "\n",
    "mask_hsv, labels_hsv = segment_blood_hsv(test_img, debug_show=True)\n",
    "\n",
    "#Result\n",
    "show_safe_preview(test_row[\"filepath\"])\n",
    "print(\"Pattern:\", test_row[\"pattern_id\"], \"| Label:\", test_row[\"label\"])\n",
    "print(\"Stains detected (Variant B):\", labels_hsv.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb6d07",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6 — Stain-level feature extraction\n",
    "\n",
    "def extract_stain_features_variantB(\n",
    "    img_bgr: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    pattern_id: str\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    props = regionprops(labels, intensity_image=v)\n",
    "\n",
    "    if len(props) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # First pass: get pattern centroid (area-weighted)\n",
    "    centroids = []\n",
    "    areas = []\n",
    "    for reg in props:\n",
    "        if reg.area < 5:\n",
    "            continue\n",
    "        centroids.append(reg.centroid)  # (row, col)\n",
    "        areas.append(reg.area)\n",
    "\n",
    "    if not centroids:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    centroids = np.array(centroids)\n",
    "    areas = np.array(areas)\n",
    "    pattern_cy = (centroids[:, 0] * areas).sum() / areas.sum()\n",
    "    pattern_cx = (centroids[:, 1] * areas).sum() / areas.sum()\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for reg in props:\n",
    "        if reg.area < 5:\n",
    "            continue\n",
    "\n",
    "        major = reg.major_axis_length or 0.0\n",
    "        minor = reg.minor_axis_length or 0.0\n",
    "        if major <= 0 or minor <= 0:\n",
    "            continue\n",
    "\n",
    "        # Shape filters (similar spirit to paper)\n",
    "        if reg.eccentricity <= 0.3:\n",
    "            continue\n",
    "\n",
    "        aspect = minor / major\n",
    "        if aspect <= 0:\n",
    "            continue\n",
    "        if aspect < (np.pi / 18.0):\n",
    "            continue\n",
    "\n",
    "        if reg.solidity < 0.75:\n",
    "            continue\n",
    "\n",
    "        filled_area = reg.filled_area\n",
    "        ellipse_area = (np.pi * major * minor) / 4.0\n",
    "        if filled_area <= 0 or ellipse_area <= 0:\n",
    "            continue\n",
    "        if (minor ** 2 / filled_area) > 1:\n",
    "            continue\n",
    "        if (reg.area / filled_area) < 0.95:\n",
    "            continue\n",
    "\n",
    "        cy, cx = reg.centroid\n",
    "        dx = cx - pattern_cx\n",
    "        dy = cy - pattern_cy\n",
    "        dist_to_centroid = math.sqrt(dx * dx + dy * dy)\n",
    "\n",
    "        major_adj = major * (reg.area / ellipse_area)\n",
    "        impact_angle = aspect  # like paper's minor/major\n",
    "        adjusted_impact_angle = minor / major_adj if major_adj > 0 else np.nan\n",
    "\n",
    "        # Colour stats inside stain\n",
    "        region_mask = (labels == reg.label)\n",
    "        h_vals = h[region_mask]\n",
    "        s_vals = s[region_mask]\n",
    "        v_vals = v[region_mask]\n",
    "\n",
    "        rows.append({\n",
    "            \"pattern_id\": pattern_id,\n",
    "            \"stain_id\": reg.label,\n",
    "            \"area\": reg.area,\n",
    "            \"major_axis\": major,\n",
    "            \"minor_axis\": minor,\n",
    "            \"eccentricity\": reg.eccentricity,\n",
    "            \"solidity\": reg.solidity,\n",
    "            \"centroid_x\": float(cx),\n",
    "            \"centroid_y\": float(cy),\n",
    "            \"orientation\": float(reg.orientation),  # radians\n",
    "            \"impact_angle\": float(impact_angle),\n",
    "            \"adjusted_impact_angle\": float(adjusted_impact_angle),\n",
    "            \"distance_to_centroid\": float(dist_to_centroid),\n",
    "            \"mean_intensity\": float(reg.mean_intensity),\n",
    "            \"intensity_stddev\": float(np.std(reg.intensity_image[reg.image])),\n",
    "            \"hue_mean\": float(np.mean(h_vals)),\n",
    "            \"hue_std\": float(np.std(h_vals)),\n",
    "            \"sat_mean\": float(np.mean(s_vals)),\n",
    "            \"sat_std\": float(np.std(s_vals)),\n",
    "            \"val_mean\": float(np.mean(v_vals)),\n",
    "            \"val_std\": float(np.std(v_vals)),\n",
    "            \"shade\": float(np.mean(v_vals)),\n",
    "            \"evenness\": float(np.std(v_vals)),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "stains_test = extract_stain_features_variantB(test_img, labels_hsv, test_row[\"pattern_id\"])\n",
    "print(\"Stain table for first image:\")\n",
    "stains_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a0d52c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7 — Pattern level global + local features\n",
    "\n",
    "def circular_variance_radians(angles: np.ndarray) -> float:\n",
    "    if len(angles) == 0:\n",
    "        return np.nan\n",
    "    s = np.sin(angles).mean()\n",
    "    c = np.cos(angles).mean()\n",
    "    R = math.sqrt(s * s + c * c)\n",
    "    return 1.0 - R\n",
    "\n",
    "def compute_global_features_variantB(df: pd.DataFrame) -> Dict[str, float]:\n",
    "    cols = [\n",
    "        \"area\", \"major_axis\", \"minor_axis\", \"solidity\",\n",
    "        \"impact_angle\", \"adjusted_impact_angle\", \"distance_to_centroid\",\n",
    "        \"mean_intensity\", \"intensity_stddev\",\n",
    "        \"hue_mean\", \"hue_std\", \"sat_mean\", \"sat_std\",\n",
    "        \"val_mean\", \"val_std\", \"shade\", \"evenness\"\n",
    "    ]\n",
    "    out = {}\n",
    "    n = len(df)\n",
    "\n",
    "    out[\"num_stains\"] = float(n)\n",
    "\n",
    "    for c in cols:\n",
    "        if n and c in df.columns:\n",
    "            out[f\"{c}_mean\"] = float(df[c].mean())\n",
    "            out[f\"{c}_std\"]  = float(df[c].std())\n",
    "        else:\n",
    "            out[f\"{c}_mean\"] = 0.0\n",
    "            out[f\"{c}_std\"]  = 0.0\n",
    "\n",
    "    if n and \"orientation\" in df.columns:\n",
    "        out[\"orientation_cvar\"] = float(\n",
    "            circular_variance_radians(df[\"orientation\"].to_numpy())\n",
    "        )\n",
    "    else:\n",
    "        out[\"orientation_cvar\"] = 0.0\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_local_features(df: pd.DataFrame,\n",
    "                           n_rings: int = 40,\n",
    "                           n_vbins: int = 40) -> Dict[str, float]:\n",
    "    out = {}\n",
    "    if len(df) == 0:\n",
    "        out[\"fract_stains_ring_15_25\"] = 0.0\n",
    "        out[\"fract_stains_ring_25_35\"] = 0.0\n",
    "        out[\"ring_with_max_stains\"] = -1\n",
    "        out[\"fract_stains_vbin_25_35\"] = 0.0\n",
    "        out[\"vbin_with_max_stains\"] = -1\n",
    "        return out\n",
    "\n",
    "    # radial rings based on distance\n",
    "    d = df[\"distance_to_centroid\"].to_numpy()\n",
    "    if np.nanmax(d) == 0:\n",
    "        d_norm = np.zeros_like(d)\n",
    "    else:\n",
    "        d_norm = d / np.nanmax(d)\n",
    "\n",
    "    ring_idx = np.floor(d_norm * n_rings).astype(int)\n",
    "    ring_idx = np.clip(ring_idx, 0, n_rings - 1)\n",
    "    ring_counts = np.bincount(ring_idx, minlength=n_rings)\n",
    "    total = ring_counts.sum()\n",
    "    if total == 0:\n",
    "        total = 1\n",
    "\n",
    "    r15_25 = ring_counts[15:25].sum()\n",
    "    r25_35 = ring_counts[25:35].sum()\n",
    "\n",
    "    out[\"fract_stains_ring_15_25\"] = r15_25 / total\n",
    "    out[\"fract_stains_ring_25_35\"] = r25_35 / total\n",
    "    out[\"ring_with_max_stains\"] = int(np.argmax(ring_counts))\n",
    "\n",
    "    # vertical bins based on centroid\n",
    "    y = df[\"centroid_y\"].to_numpy()\n",
    "    if np.nanmax(y) == np.nanmin(y):\n",
    "        y_norm = np.zeros_like(y)\n",
    "    else:\n",
    "        y_norm = (y - np.nanmin(y)) / (np.nanmax(y) - np.nanmin(y))\n",
    "\n",
    "    vbin_idx = np.floor(y_norm * n_vbins).astype(int)\n",
    "    vbin_idx = np.clip(vbin_idx, 0, n_vbins - 1)\n",
    "    v_counts = np.bincount(vbin_idx, minlength=n_vbins)\n",
    "    v_total = v_counts.sum()\n",
    "    if v_total == 0:\n",
    "        v_total = 1\n",
    "\n",
    "    v25_35 = v_counts[25:35].sum()\n",
    "\n",
    "    out[\"fract_stains_vbin_25_35\"] = v25_35 / v_total\n",
    "    out[\"vbin_with_max_stains\"] = int(np.argmax(v_counts))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def pattern_features_variantB(img_bgr: np.ndarray,\n",
    "                              pattern_id: str) -> Dict[str, float]:\n",
    "\n",
    "    mask, labels = segment_blood_hsv(img_bgr)\n",
    "    stains_df = extract_stain_features_variantB(img_bgr, labels, pattern_id)\n",
    "\n",
    "    global_feats = compute_global_features_variantB(stains_df)\n",
    "    local_feats = compute_local_features(stains_df)\n",
    "\n",
    "    feat = {**global_feats, **local_feats}\n",
    "    feat[\"pattern_id\"] = pattern_id\n",
    "    feat[\"valid_stains\"] = float(len(stains_df))\n",
    "    return feat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e33ce1f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 8 — Build images_df from the WHOLE DATA_ROOT using filename prefixes\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "IMG_EXTS = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tif\", \"*.tiff\")\n",
    "\n",
    "def infer_label_from_filename(filename: str):\n",
    "\n",
    "    name = os.path.basename(filename)\n",
    "    upper = name.upper()\n",
    "\n",
    "    if upper.startswith(\"GS-\"):\n",
    "        return \"gunshot\", 0\n",
    "    else:\n",
    "        return \"impact\", 1\n",
    "\n",
    "\n",
    "def collect_images_from_root(root: str) -> pd.DataFrame:\n",
    "    records = []\n",
    "    # recursive=True picks up subfolders like 'Gunshot'\n",
    "    for ext in IMG_EXTS:\n",
    "        pattern = os.path.join(root, \"**\", ext)\n",
    "        files = glob.glob(pattern, recursive=True)\n",
    "        for f in files:\n",
    "            label_str, label_int = infer_label_from_filename(f)\n",
    "            pid = os.path.splitext(os.path.basename(f))[0]\n",
    "            records.append(\n",
    "                {\n",
    "                    \"pattern_id\": pid,\n",
    "                    \"filepath\": f,\n",
    "                    \"label\": label_str,\n",
    "                    \"label_int\": label_int,\n",
    "                }\n",
    "            )\n",
    "    df = pd.DataFrame(records)\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\n",
    "            f\"No images found in {root}. Check DATA_ROOT or extensions.\"\n",
    "        )\n",
    "    return df\n",
    "\n",
    "images_df = collect_images_from_root(DATA_ROOT)\n",
    "\n",
    "print(\"Total images found in DATA_ROOT (all folders):\", len(images_df))\n",
    "print(\"By class (inferred from filename):\")\n",
    "print(images_df[\"label\"].value_counts())\n",
    "images_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0280069",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 9 — FULL SIZE dataset with every image\n",
    "\n",
    "def build_dataset_variantB_full(images_df: pd.DataFrame) -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    rows = []\n",
    "    y_list = []\n",
    "    \n",
    "    total = len(images_df)\n",
    "    print(\"\\n Building Dataset\")\n",
    "    print(f\"Total images to process: {total}\\n\")\n",
    "\n",
    "    for idx, row in images_df.iterrows():\n",
    "        pid = str(row[\"pattern_id\"])\n",
    "        path = row[\"filepath\"]\n",
    "        label_int = int(row[\"label_int\"])\n",
    "\n",
    "        print(f\"[{idx+1}/{total}] {pid} | {path}\")\n",
    "\n",
    "       \n",
    "        try:\n",
    "            img = load_image_bgr(path)        \n",
    "            orig_h, orig_w = img.shape[:2]\n",
    "        except Exception as e:\n",
    "            print(f\" [LOAD ERROR] Could not load: {path}\")\n",
    "            print(f\"  Reason: {e}\\n\")\n",
    "            continue\n",
    "\n",
    "        # Feature extraction\n",
    "        try:\n",
    "            feats = pattern_features_variantB(img, pid)\n",
    "\n",
    "            # save size columns\n",
    "            feats[\"orig_h\"] = orig_h\n",
    "            feats[\"orig_w\"] = orig_w\n",
    "            feats[\"resized_h\"] = orig_h\n",
    "            feats[\"resized_w\"] = orig_w\n",
    "\n",
    "            rows.append(feats)\n",
    "            y_list.append(label_int)\n",
    "\n",
    "            print(f\"Features extracted.\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[FEATURE ERROR] pattern_id={pid}\")\n",
    "            print(f\"Reason: {e}\\n\")\n",
    "            continue\n",
    "\n",
    "    #Build DataFrame\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"No images processed successfully. Check errors\")\n",
    "\n",
    "    X_df = pd.DataFrame(rows)\n",
    "    if \"pattern_id\" in X_df.columns:\n",
    "        X_df = X_df.set_index(\"pattern_id\")\n",
    "\n",
    "    y = np.array(y_list, dtype=int)\n",
    "\n",
    "  \n",
    "    print(\"\\n FULL-SIZE Dataset Build Complete\")\n",
    "    print(f\"Total images in dataset: {total}\")\n",
    "    print(f\"Successfully processed:  {len(X_df)}\")\n",
    "    print(f\"Failed images:           {total - len(X_df)}\")\n",
    "\n",
    "    if len(X_df) != total:\n",
    "        print(\"\\nSome images had errors. Scroll above for detailed logs.\")\n",
    "\n",
    "\n",
    "\n",
    "    return X_df, y\n",
    "\n",
    "\n",
    "X_B, y_B = build_dataset_variantB_full(images_df)\n",
    "print(\"Dataset shape:\", X_B.shape, \"labels:\", y_B.shape)\n",
    "X_B.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c2bd6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Saving as CSV\n",
    "X_B_csv = X_B.copy()\n",
    "X_B_csv[\"label_int\"] = y_B\n",
    "\n",
    "csv_path = \"features.csv\"\n",
    "X_B_csv.to_csv(csv_path, index=True)\n",
    "\n",
    "print(f\"Saved features + size info to: {csv_path}\")\n",
    "display(X_B_csv.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b293862b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Path to the CSV \n",
    "CSV_PATH = r\"C:\\BPA\\features.csv\" \n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f713a0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ONLY GS- is gunshot. Everything else is impact.\n",
    "if \"pattern_id\" not in df.columns:\n",
    "    if df.index.name is not None:\n",
    "        df = df.reset_index().rename(columns={df.index.name: \"pattern_id\"})\n",
    "    else:\n",
    "        raise RuntimeError(\"No 'pattern_id' column or index in CSV – please check the file.\")\n",
    "\n",
    "def infer_label_from_pattern_id(pid: str):\n",
    "\n",
    "    name = str(pid).upper()\n",
    "    if name.startswith(\"GS-\"):\n",
    "        return \"gunshot\", 0\n",
    "    else:\n",
    "        return \"impact\", 1\n",
    "\n",
    "labels_str = []\n",
    "labels_int = []\n",
    "\n",
    "for pid in df[\"pattern_id\"]:\n",
    "    s, i = infer_label_from_pattern_id(pid)\n",
    "    labels_str.append(s)\n",
    "    labels_int.append(i)\n",
    "\n",
    "df[\"label\"] = labels_str\n",
    "df[\"label_int\"] = labels_int\n",
    "\n",
    "LABEL_TO_INT = {\"gunshot\": 0, \"impact\": 1}\n",
    "INT_TO_LABEL = {v: k for k, v in LABEL_TO_INT.items()}\n",
    "\n",
    "print(\"Label distribution (recomputed from GS rule):\")\n",
    "unique, counts = np.unique(df[\"label_int\"].values, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"  int {u} -> {INT_TO_LABEL[u]:8s} : {c} samples\")\n",
    "\n",
    "# Non-feature columns to exclude\n",
    "NON_FEATURE_COLS = {\"pattern_id\", \"label\", \"label_int\"}\n",
    "\n",
    "feature_cols = [c for c in df.columns if c not in NON_FEATURE_COLS]\n",
    "\n",
    "X_B = df[feature_cols].copy()\n",
    "y_B = df[\"label_int\"].astype(int).values\n",
    "\n",
    "print(\"X_B shape:\", X_B.shape)\n",
    "print(\"y_B shape:\", y_B.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa111a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#XG Boost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "pos_label = LABEL_TO_INT[\"gunshot\"]\n",
    "n_pos = np.sum(y_B == pos_label)\n",
    "n_neg = np.sum(y_B != pos_label)\n",
    "print(f\"Gunshot count: {n_pos}, Impact count: {n_neg}\")\n",
    "\n",
    "scale_pos_weight = n_neg / max(1, n_pos)\n",
    "print(\"scale_pos_weight:\", scale_pos_weight)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_B.values.astype(float),\n",
    "    y_B,\n",
    "    test_size=0.25,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y_B,\n",
    ")\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=scale_pos_weight,  \n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)  \n",
    "\n",
    "print(\"\\nXGBoost classification report:\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred,\n",
    "    target_names=[INT_TO_LABEL[0], INT_TO_LABEL[1]],\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5e0eee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Random forest check \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=None,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\",  \n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"\\nRandomForest classification report (balanced):\")\n",
    "print(classification_report(\n",
    "    y_test,\n",
    "    y_pred_rf,\n",
    "    target_names=[INT_TO_LABEL[0], INT_TO_LABEL[1]],\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431b7243",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Train on full dataset\n",
    "xgb_model_final = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.05,\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    ")\n",
    "\n",
    "xgb_model_final.fit(X_B.values.astype(float), y_B)\n",
    "\n",
    "MODEL_DIR = Path(r\"C:\\BPA\\model\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_PATH = MODEL_DIR / \"variantB_xgb.joblib\"\n",
    "\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"model\": xgb_model_final,\n",
    "        \"feature_cols\": feature_cols,\n",
    "        \"label_to_int\": LABEL_TO_INT,\n",
    "        \"int_to_label\": INT_TO_LABEL,\n",
    "    },\n",
    "    MODEL_PATH,\n",
    ")\n",
    "\n",
    "print(\"Saved model to:\", MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951ca31",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 13 — Contingency: superpixel-based mask refinement\n",
    "\n",
    "def refine_mask_with_superpixels(img_bgr: np.ndarray,\n",
    "                                 seed_mask: np.ndarray,\n",
    "                                 n_segments: int = 400,\n",
    "                                 compactness: float = 10.0,\n",
    "                                 prob_thresh: float = 0.5) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Contingency segmentation:\n",
    "      - Use seed_mask (from grayscale baseline) as weak labels.\n",
    "      - Run SLIC superpixels.\n",
    "      - Train RF on superpixel-level features \n",
    "      - Predict stain probability per superpixel.\n",
    "      - Threshold + morphology → refined mask.\n",
    "    \"\"\"\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    img_float = img_as_float(img_rgb)\n",
    "\n",
    "    sp_labels = slic(\n",
    "        img_float,\n",
    "        n_segments=n_segments,\n",
    "        compactness=compactness,\n",
    "        start_label=0\n",
    "    )\n",
    "\n",
    "    hsv = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    seed_bool = seed_mask.astype(bool)\n",
    "    H, W = seed_mask.shape\n",
    "    yy, xx = np.indices(seed_mask.shape)\n",
    "\n",
    "    feat_list = []\n",
    "    seed_labels = []\n",
    "\n",
    "    for lab_id in np.unique(sp_labels):\n",
    "        mask_sp = (sp_labels == lab_id)\n",
    "        if mask_sp.sum() == 0:\n",
    "            continue\n",
    "\n",
    "        h_vals = h[mask_sp]\n",
    "        s_vals = s[mask_sp]\n",
    "        v_vals = v[mask_sp]\n",
    "\n",
    "        h_mean = float(np.mean(h_vals))\n",
    "        s_mean = float(np.mean(s_vals))\n",
    "        v_mean = float(np.mean(v_vals))\n",
    "\n",
    "        x_norm = float(xx[mask_sp].mean() / W)\n",
    "        y_norm = float(yy[mask_sp].mean() / H)\n",
    "\n",
    "        pos_frac = seed_bool[mask_sp].mean()\n",
    "        if pos_frac >= 0.8:\n",
    "            label_sp = 1\n",
    "        elif pos_frac <= 0.05:\n",
    "            label_sp = 0\n",
    "        else:\n",
    "            label_sp = -1  # unlabeled\n",
    "\n",
    "        feat_list.append([h_mean, s_mean, v_mean, x_norm, y_norm])\n",
    "        seed_labels.append(label_sp)\n",
    "\n",
    "    feats = np.array(feat_list, dtype=float)\n",
    "    seed_labels = np.array(seed_labels, dtype=int)\n",
    "\n",
    "    mask_labeled = seed_labels >= 0\n",
    "    if mask_labeled.sum() < 5:\n",
    "        # not enough seeds; just return the seed mask\n",
    "        return seed_mask\n",
    "\n",
    "    X_train = feats[mask_labeled]\n",
    "    y_train = seed_labels[mask_labeled]\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    probs = clf.predict_proba(feats)[:, 1]\n",
    "\n",
    "    prob_map = np.zeros_like(seed_mask, dtype=float)\n",
    "    idx = 0\n",
    "    for lab_id in np.unique(sp_labels):\n",
    "        mask_sp = (sp_labels == lab_id)\n",
    "        prob_map[mask_sp] = probs[idx]\n",
    "        idx += 1\n",
    "\n",
    "    refined_bool = prob_map >= prob_thresh\n",
    "    refined_bool = binary_opening(refined_bool, disk(1))\n",
    "    refined_bool = binary_closing(refined_bool, disk(2))\n",
    "\n",
    "    refined_mask = refined_bool.astype(np.uint8) * 255\n",
    "    return refined_mask\n",
    "\n",
    "print(\"Contingency refinement function ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70a8487",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 14- Load trained Variant B model     \n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(r\"C:\\BPA\\model\\variantB_xgb.joblib\")  \n",
    "\n",
    "bundle = joblib.load(MODEL_PATH)\n",
    "use_model = bundle[\"model\"]\n",
    "use_feat_cols = bundle[\"feature_cols\"]\n",
    "LABEL_TO_INT = bundle[\"label_to_int\"]\n",
    "INT_TO_LABEL = bundle[\"int_to_label\"]\n",
    "\n",
    "print(f\"Loaded trained Variant B model bundle from: {MODEL_PATH}\")\n",
    "print(\"LABEL_TO_INT:\", LABEL_TO_INT)\n",
    "print(\"INT_TO_LABEL:\", INT_TO_LABEL)\n",
    "print(f\"Number of features: {len(use_feat_cols)}\")\n",
    "\n",
    "CONF_THRESHOLD = 0.75\n",
    "print(f\"Contingency will trigger if max probability < {CONF_THRESHOLD}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c67a9c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 15 - Single image prediction using Variant B + contingency\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def predict_with_model(\n",
    "    img_path: str,\n",
    "    model,\n",
    "    feature_cols,\n",
    "    conf_threshold: float = CONF_THRESHOLD\n",
    "):\n",
    "\n",
    "    \n",
    "    img_bgr = load_image_bgr(img_path)\n",
    "    pattern_id = Path(img_path).stem\n",
    "\n",
    "    #features from variant B \n",
    "    X_row_B, mask_B = features_for_inference_variantB(\n",
    "        img_bgr=img_bgr,\n",
    "        pattern_id=pattern_id,\n",
    "        feature_cols=feature_cols\n",
    "    )\n",
    "\n",
    "    # prediction\n",
    "    probs = model.predict_proba(X_row_B.values.astype(float))[0]\n",
    "    max_idx = int(np.argmax(probs))\n",
    "    max_prob = float(probs[max_idx])\n",
    "    main_label = INT_TO_LABEL[max_idx]   \n",
    "\n",
    "    print(f\"[Variant B] {pattern_id} → {main_label} (conf = {max_prob:.3f})\")\n",
    "\n",
    "    if max_prob >= conf_threshold:\n",
    "        return main_label, max_prob, dict(zip(INT_TO_LABEL.values(), probs)), {\n",
    "            \"used_contingency\": False,\n",
    "            \"mask_main\": mask_B,\n",
    "            \"mask_contingency\": None,\n",
    "        }\n",
    "\n",
    "    # contigency\n",
    "    print(f\"Low confidence (< {conf_threshold}), running contingency...\")\n",
    "\n",
    "    seed_mask = segment_gray_baseline(img_bgr, min_area=30, scale_factor=1.0)\n",
    "    refined_mask = refine_mask_with_superpixels(img_bgr, seed_mask)\n",
    "\n",
    "    labels_refined = sk_label(refined_mask > 0)\n",
    "    stains_df = extract_stain_features_variantB(img_bgr, labels_refined, pattern_id)\n",
    "\n",
    "    gl = compute_global_features_variantB(stains_df)\n",
    "    loc = compute_local_features(stains_df)\n",
    "    gl[\"valid_stains\"] = len(stains_df)\n",
    "    feat_all_C = {**gl, **loc}\n",
    "\n",
    "    row_C = {c: 0.0 for c in feature_cols}\n",
    "    for k, v in feat_all_C.items():\n",
    "        if k in row_C:\n",
    "            row_C[k] = float(v)\n",
    "\n",
    "    X_row_C = pd.DataFrame([row_C], columns=feature_cols)\n",
    "    probs_C = model.predict_proba(X_row_C.values.astype(float))[0]\n",
    "\n",
    "    max_idx_C = int(np.argmax(probs_C))\n",
    "    max_prob_C = float(probs_C[max_idx_C])\n",
    "    cont_label = INT_TO_LABEL[max_idx_C]   \n",
    "\n",
    "    print(f\"[Contingency] {pattern_id} → {cont_label} (conf = {max_prob_C:.3f})\")\n",
    "\n",
    "    return cont_label, max_prob_C, dict(zip(INT_TO_LABEL.values(), probs_C)), {\n",
    "        \"used_contingency\": True,\n",
    "        \"mask_main\": mask_B,\n",
    "        \"mask_contingency\": refined_mask,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc97554e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 16 - Variant B inference feature builder\n",
    "\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def features_for_inference_variantB(\n",
    "    img_bgr: np.ndarray,\n",
    "    pattern_id: str,\n",
    "    feature_cols: List[str]\n",
    ") -> Tuple[pd.DataFrame, np.ndarray]:\n",
    " \n",
    "    #Segmentation of blood in HSV \n",
    "    mask_B, labels_B = segment_blood_hsv(img_bgr)\n",
    "\n",
    "    #Extracting stain level features\n",
    "    stains_df = extract_stain_features_variantB(img_bgr, labels_B, pattern_id)\n",
    "\n",
    "    #Aggregating to global + local pattern features\n",
    "    gl = compute_global_features_variantB(stains_df)\n",
    "    loc = compute_local_features(stains_df)\n",
    "    gl[\"valid_stains\"] = len(stains_df)\n",
    "\n",
    "    feat_all = {**gl, **loc}\n",
    "\n",
    "    # Align to training feature columns (filling missing values with 0.0)\n",
    "    row = {c: 0.0 for c in feature_cols}\n",
    "    for k, v in feat_all.items():\n",
    "        if k in row:\n",
    "            row[k] = float(v)\n",
    "\n",
    "    X_row = pd.DataFrame([row], columns=feature_cols)\n",
    "\n",
    "    return X_row, mask_B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eba21c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 17 - Test on one image\n",
    "\n",
    "test_path = r\"C:\\BPA\\Test2.jpg\"  \n",
    "\n",
    "label, conf, prob_dict, extra = predict_with_model(\n",
    "    img_path=test_path,\n",
    "    model=use_model,\n",
    "    feature_cols=use_feat_cols,\n",
    "    conf_threshold=CONF_THRESHOLD,\n",
    ")\n",
    "\n",
    "print(\"\\nDecision:\")\n",
    "print(f\"  Label      : {label}\")\n",
    "print(f\"  Confidence : {conf:.3f}\")\n",
    "print(f\"  Used contingency {extra['used_contingency']}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
